# Logistic Regression vs LSTM for Sentiment Analysis

NLP 202 Assignment 1 - Comparative study of Logistic Regression and LSTM models on IMDB movie reviews dataset.

## ğŸ¯ Project Overview

This project implements and compares two neural network architectures for binary sentiment classification:
- **Logistic Regression** with embeddings
- **LSTM (Long Short-Term Memory)** with embeddings

Both models use minibatching, proper padding/masking, and comprehensive hyperparameter tuning.

## ğŸ“Š Key Results

| Model | Test Accuracy | Val Accuracy | Best Batch Size | Best LR |
|-------|--------------|--------------|-----------------|---------|
| **Logistic Regression** | **88.14%** | 87.00% | 64 | 1e-3 |
| **LSTM** | 83.95% | 76.62% | 64 | 1e-3 |

**Key Finding:** The simpler Logistic Regression model outperformed LSTM by 4.19% on test accuracy!

## ğŸ“ Repository Structure

```
â”œâ”€â”€ assignment_solution.py          # Main implementation
â”œâ”€â”€ error_analysis.py               # Error analysis and visualization
â”œâ”€â”€ README.md                       # Detailed documentation
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ outputs/                        # Model predictions
â”‚   â”œâ”€â”€ lr_valid_predictions.npy
â”‚   â”œâ”€â”€ lr_test_predictions.npy
â”‚   â”œâ”€â”€ lstm_valid_predictions.npy
â”‚   â””â”€â”€ lstm_test_predictions.npy
â””â”€â”€ overleaf_report/               # LaTeX report
    â”œâ”€â”€ main.tex
    â””â”€â”€ plots/                      # 8 visualization plots
```

## ğŸš€ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/pratirvce/LR-vs-LSTM.git
cd LR-vs-LSTM
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

### 3. Download IMDB Dataset
```bash
wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
tar -xzf aclImdb_v1.tar.gz
```

### 4. Run Training
```bash
python assignment_solution.py
```

### 5. Run Error Analysis
```bash
python error_analysis.py
```

## ğŸ”§ Implementation Features

### Data Processing
- **Tokenization:** spaCy `en_core_web_sm`
- **Vocabulary:** Top 25,000 words
- **Dataset:** IMDB 50K reviews (20K train, 5K val, 25K test)

### Logistic Regression Model
- Embedding layer (100-dim)
- Average pooling with masking
- Linear output layer
- BCEWithLogitsLoss

### LSTM Model
- Embedding layer (100-dim)
- LSTM layer (128 hidden units)
- pack_padded_sequence / pad_packed_sequence
- Average pooling with masking
- Dropout (0.5)
- Linear output layer

### Technical Highlights
âœ… Proper padding using `pad_sequence()`  
âœ… Masking to exclude padding from computations  
âœ… LSTM packing/unpacking for efficiency  
âœ… Model correctness verification  
âœ… Comprehensive hyperparameter tuning  
âœ… Batch sizes tested: [16, 32, 64, 128]  
âœ… Learning rates tested: [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]

## ğŸ“ˆ Experimental Results

### Hyperparameter Tuning Results

**Batch Size Impact:**
- Larger batch sizes â†’ Faster training
- Optimal: 64 for both models
- Best trade-off between speed and accuracy

**Learning Rate Impact:**
- 1e-3 optimal for both models
- Too low (1e-4): Slow convergence
- Too high (1e-2): Unstable training

### Model Comparison

**Logistic Regression:**
- âœ… Higher accuracy (88.14%)
- âœ… Faster training (~68s/epoch)
- âœ… Simpler architecture
- âœ… Better generalization

**LSTM:**
- ğŸ“Š Lower accuracy (83.95%)
- â±ï¸ Slower training (~168s/epoch)
- ğŸ”„ More complex architecture
- âš ï¸ Potential overfitting on validation set

## ğŸ“Š Visualizations

The project includes 8 comprehensive plots:
1. Training time vs batch size (both models)
2. Accuracy vs batch size (both models)
3. Accuracy vs learning rate (both models)
4. Confusion matrices comparison
5. Metrics comparison bar chart

All plots are available in `overleaf_report/plots/`

## ğŸ” Error Analysis

### Key Observations

**Logistic Regression Errors:**
- False negatives: Sarcasm, subtle negativity
- False positives: Mixed reviews, plot spoilers

**LSTM Errors:**
- More false negatives overall
- Struggles with: Long reviews, complex narratives
- Better at: Capturing sequential patterns (when correct)

See `error_analysis.py` for detailed examples and analysis.

## ğŸ“ Report

A comprehensive LaTeX report is available in `overleaf_report/main.tex` covering:
- Model architectures
- Correctness verification
- Hyperparameter tuning experiments
- Performance comparison
- Error analysis
- Conclusions

## ğŸ› ï¸ Technologies Used

- **PyTorch** - Deep learning framework
- **spaCy** - NLP tokenization
- **NumPy** - Numerical computing
- **Matplotlib** - Visualization
- **scikit-learn** - Metrics
- **tqdm** - Progress bars

## ğŸ“„ License

This project was created as coursework for NLP 202.

## ğŸ‘¤ Author

Pratibha Revankar  
GitHub: [@pratirvce](https://github.com/pratirvce)

## ğŸ™ Acknowledgments

- IMDB dataset: [Maas et al., 2011](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)
- Course: NLP 202 - Natural Language Processing
- Assignment focus: Understanding minibatching in PyTorch

## ğŸ“š References

1. Maas, A. L., et al. (2011). Learning Word Vectors for Sentiment Analysis. ACL.
2. PyTorch Documentation: https://pytorch.org/docs/
3. spaCy Documentation: https://spacy.io/

---

â­ If you find this project useful, please consider giving it a star!
