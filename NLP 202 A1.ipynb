{"cells":[{"cell_type":"code","execution_count":null,"id":"df30e566","metadata":{"id":"df30e566"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","import spacy\n","from collections import Counter\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score\n","\n"]},{"cell_type":"code","execution_count":null,"id":"316ca7da","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7634,"status":"ok","timestamp":1769030684637,"user":{"displayName":"Jeffrey Flanigan","userId":"04549483446617689827"},"user_tz":480},"id":"316ca7da","outputId":"b23290ab-fc81-426f-d040-61a9f53e0499"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2026-01-21 21:24:36--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n","Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84125825 (80M) [application/x-gzip]\n","Saving to: ‘aclImdb_v1.tar.gz’\n","\n","aclImdb_v1.tar.gz   100%[===================>]  80.23M  16.9MB/s    in 7.6s    \n","\n","2026-01-21 21:24:44 (10.6 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n","\n"]}],"source":["!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n"]},{"cell_type":"code","execution_count":null,"id":"d82731a8","metadata":{"id":"d82731a8"},"outputs":[],"source":["!tar -xzf aclImdb_v1.tar.gz\n"]},{"cell_type":"code","execution_count":null,"id":"9a558dc4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7187,"status":"ok","timestamp":1769013289495,"user":{"displayName":"Jeffrey Flanigan","userId":"04549483446617689827"},"user_tz":480},"id":"9a558dc4","outputId":"44ed8f2b-05de-49d7-e0a9-e59e44e010da"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting en-core-web-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":null,"id":"603b4dfa","metadata":{"id":"603b4dfa"},"outputs":[],"source":["# Set random seed for reproducibility\n","SEED = 1234\n","torch.manual_seed(SEED)\n","random.seed(SEED)\n","np.random.seed(SEED)\n","\n","# Load spaCy tokenizer\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","PAD_IDX = 0\n","UNK_IDX = 1\n","\n","def tokenize(text):\n","    return [token.text.lower() for token in nlp(text)]\n","\n","# Build vocabulary\n","def build_vocab(texts, max_vocab_size=25_000):\n","    counter = Counter(token for text in texts for token in tokenize(text))\n","    vocab = {word: idx + 2 for idx, (word, _) in enumerate(counter.most_common(max_vocab_size))}\n","    vocab[\"<pad>\"] = PAD_IDX\n","    vocab[\"<unk>\"] = UNK_IDX\n","    return vocab\n","\n","# Numericalize text\n","def numericalize(texts, vocab):\n","    return [[vocab.get(token, UNK_IDX) for token in tokenize(text)] for text in texts]\n","\n","# Load IMDB dataset\n","def load_imdb_data(data_dir):\n","    texts, labels = [], []\n","    for label_type in [\"pos\", \"neg\"]:\n","        folder = f\"{data_dir}/{label_type}\"\n","        for file in os.listdir(folder):\n","            with open(f\"{folder}/{file}\", \"r\", encoding=\"utf-8\") as f:\n","                texts.append(f.read())\n","                labels.append(1 if label_type == \"pos\" else 0)\n","    return texts, labels\n"]},{"cell_type":"code","execution_count":null,"id":"a3cd1a30","metadata":{"id":"a3cd1a30"},"outputs":[],"source":["class IMDBDataset(Dataset):\n","    def __init__(self, texts, labels, vocab):\n","        self.texts = numericalize(texts, vocab)\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n","\n","def collate_fn(batch):\n","    texts, labels = zip(*batch)\n","    lengths = torch.tensor([len(text) for text in texts])\n","    padded_texts = pad_sequence(texts, batch_first=True, padding_value=PAD_IDX)\n","    labels = torch.tensor(labels, dtype=torch.float)\n","    return padded_texts, labels, lengths\n"]},{"cell_type":"code","execution_count":null,"id":"991d58bc","metadata":{"id":"991d58bc"},"outputs":[],"source":["class LogisticRegression(nn.Module):\n","    def __init__(self, vocab_size, embed_dim):\n","        super(LogisticRegression, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n","        self.fc = nn.Linear(embed_dim, 1)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)  # Shape: [batch_size, seq_len, embed_dim]\n","        pooled = embedded.mean(dim=1)  # Average over the sequence length\n","        return self.fc(pooled).squeeze(1)  # Shape: [batch_size]\n"]},{"cell_type":"code","execution_count":null,"id":"a2f0d011","metadata":{"id":"a2f0d011"},"outputs":[],"source":["def train_model(model, dataloader, optimizer, criterion, device):\n","    model.train()\n","    epoch_loss = 0\n","    for texts, labels, _ in tqdm(dataloader):\n","        texts, labels = texts.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        predictions = model(texts)\n","        loss = criterion(predictions, labels)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","    return epoch_loss / len(dataloader)\n","\n","def evaluate_model(model, dataloader, criterion, device):\n","    model.eval()\n","    epoch_loss = 0\n","    all_preds, all_labels = [], []\n","    with torch.no_grad():\n","        for texts, labels, _ in dataloader:\n","            texts, labels = texts.to(device), labels.to(device)\n","            predictions = model(texts)\n","            loss = criterion(predictions, labels)\n","            epoch_loss += loss.item()\n","            all_preds.extend(torch.round(torch.sigmoid(predictions)).cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    return epoch_loss / len(dataloader), accuracy\n"]},{"cell_type":"code","execution_count":null,"id":"e6f740c7","metadata":{"id":"e6f740c7"},"outputs":[],"source":["# Load data\n","import os\n","train_texts, train_labels = load_imdb_data(\"./aclImdb/train\")\n","test_texts, test_labels = load_imdb_data(\"./aclImdb/test\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"82632658","metadata":{"id":"82632658"},"outputs":[],"source":["\n","# Split data\n","train_texts, valid_texts = train_texts[:20000], train_texts[20000:]\n","train_labels, valid_labels = train_labels[:20000], train_labels[20000:]\n","\n","# Build vocabulary\n","vocab = build_vocab(train_texts)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"dee42997","metadata":{"colab":{"background_save":true},"id":"dee42997"},"outputs":[],"source":["# Create datasets and dataloaders\n","train_dataset = IMDBDataset(train_texts, train_labels, vocab)\n","valid_dataset = IMDBDataset(valid_texts, valid_labels, vocab)\n","test_dataset = IMDBDataset(test_texts, test_labels, vocab)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n","valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"]},{"cell_type":"code","execution_count":null,"id":"405ee250","metadata":{"colab":{"background_save":true},"id":"405ee250","outputId":"4d8b5613-bfba-4500-dadb-145aca9b9cc2"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 625/625 [00:17<00:00, 35.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.6441, Valid Loss = 0.8067, Valid Accuracy = 0.1746\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 625/625 [00:15<00:00, 39.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.5264, Valid Loss = 0.6686, Valid Accuracy = 0.5788\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 625/625 [00:15<00:00, 40.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.4083, Valid Loss = 0.5558, Valid Accuracy = 0.7046\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 625/625 [00:15<00:00, 39.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4: Train Loss = 0.3346, Valid Loss = 0.4895, Valid Accuracy = 0.7594\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 625/625 [00:15<00:00, 40.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: Train Loss = 0.2895, Valid Loss = 0.4304, Valid Accuracy = 0.8016\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 625/625 [00:15<00:00, 39.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6: Train Loss = 0.2563, Valid Loss = 0.4324, Valid Accuracy = 0.8064\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 625/625 [00:16<00:00, 38.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7: Train Loss = 0.2297, Valid Loss = 0.4098, Valid Accuracy = 0.8224\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 625/625 [00:15<00:00, 39.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8: Train Loss = 0.2076, Valid Loss = 0.4004, Valid Accuracy = 0.8286\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 625/625 [00:15<00:00, 39.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9: Train Loss = 0.1887, Valid Loss = 0.3518, Valid Accuracy = 0.8544\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 625/625 [00:15<00:00, 39.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10: Train Loss = 0.1709, Valid Loss = 0.3804, Valid Accuracy = 0.8448\n"]}],"source":["\n","# Model, optimizer, and loss function\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = LogisticRegression(len(vocab), embed_dim=100).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# Train the model\n","N_EPOCHS = 10\n","for epoch in range(N_EPOCHS):\n","    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n","    valid_loss, valid_acc = evaluate_model(model, valid_loader, criterion, device)\n","    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Valid Loss = {valid_loss:.4f}, Valid Accuracy = {valid_acc:.4f}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}